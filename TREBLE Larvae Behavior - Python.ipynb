{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3368722",
   "metadata": {},
   "source": [
    "# Basic Instructions\n",
    "\n",
    "To use TREBLE analysis, run the boxes one at a time. You can choose to run multiple in succession, but the next box will not run till the previous one completes. Certain boxes will ask you to type the names of file paths, folders, and data. These boxes will be labeled so you will know when to fill things out. There will also be boxes to save your work in \"pickle\" format as you go along. This formatted data will allow you to access the graphs and analysis later on without rerunning the whole program. The names of the pickle files will be made based on the experiment name and date you enter at the beginning of the file.\n",
    "\n",
    "After first running the analysis, once your pickle files have been saved, you can skip ahead start at the section labeled \"Analyzing UMAP Behavior Space\". Note: The \"Imports\", \"Set Up Working Directory\", \"Set Directories\", \"Set Name and Date for Data\", and \"Load Functions\" boxes must be run everytime you are performing analysis or accessing previous analysis.\n",
    "\n",
    "Original TREBLE program made in R by: Ryan A. York\n",
    "\n",
    "For any questions, please email bela.aguilar21@gmail.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebff16d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import umap # must first do: pip install umap-learn\n",
    "import community  # pip install python-louvain\n",
    "import TREBLE_Functions as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.stats import kruskal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959adc6",
   "metadata": {},
   "source": [
    "# Set Up Working Directory\n",
    "###### Enter file path in box after running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = None\n",
    "while wd == None:\n",
    "    wd = input(\"Enter path to folder where all the files are, (eg: /Users/Arnaldo/Desktop/TREBLE) : \")\n",
    "\n",
    "def check_wd(wd):\n",
    "    \"\"\"\n",
    "    Checks that the working directory is set up correctly and can be accessed\n",
    "    \"\"\"\n",
    "    if os.path.exists(wd):\n",
    "        return 'Success'\n",
    "    return 'Failed'\n",
    "\n",
    "result = check_wd(wd)\n",
    "while result == 'Failed':\n",
    "    wd = input(\"The path to folder is not correct. Enter path to folder where all the files are, (eg: /Users/Arnaldo/Desktop/TREBLE) : \")\n",
    "    result = check_wd(wd)\n",
    "    \n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447b55a",
   "metadata": {},
   "source": [
    "# Set Directories\n",
    "###### Enter the folder names of control and experimental flies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6417df",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(wd)\n",
    "\n",
    "wd = os.getcwd()\n",
    "\n",
    "directory_control = input(\"Enter folder name for the folder with control files, (eg: directory_control) : \")\n",
    "while directory_control not in files:\n",
    "    print(\"This folder is not present in\", wd)\n",
    "    directory_control = input(\"Enter folder name for the folder with control files, (eg: directory_control) : \")\n",
    "    \n",
    "directory_experimental = input(\"Enter folder name for the folder with experimental files, (eg: directory_experimental) : \")\n",
    "while directory_experimental not in files:\n",
    "    print(\"This folder is not present in\", wd)\n",
    "    directory_experimental = input(\"Enter folder name for the folder with experimental files, (eg: directory_experimental) : \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006899a5",
   "metadata": {},
   "source": [
    "# Set Name and Date for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = input(\"Please input the experiment name to be saved to data file names, (eg: MechanoCh)\").strip()\n",
    "experiment_date = input(\"Please input the date of the experiment to be saved to data file names, (eg: 06232024)\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d194fe9",
   "metadata": {},
   "source": [
    "# Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e138f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bearing\n",
    "def bearing(x1=10, y1=10, x2=3, y2=3):\n",
    "    if x1 == x2 and y1 > y2:\n",
    "        return 360\n",
    "    elif x1 == x2 and y1 < y2: # CORRECTED THIS LINE FROM R CODE\n",
    "        return 90\n",
    "    elif y1 == y2 and x1 > x2:\n",
    "        return 270\n",
    "    elif y1 == y2 and x1 < x2:\n",
    "        return 180\n",
    "    elif x1 == x2 and y1 == y2:\n",
    "        return float('nan')\n",
    "    else:\n",
    "        theta = math.atan2(x2-x1,y1-y2)\n",
    "    if theta < 0:\n",
    "        theta = theta + 2*math.pi\n",
    "    theta = math.degrees(theta)\n",
    "    return theta\n",
    "\n",
    "\n",
    "# Function to calculate within condition variance\n",
    "def within_species_variance_umap(layout, condition, extract_condition = False):\n",
    "    if extract_condition:\n",
    "        # Extract species\n",
    "        l = layout #l = layout[layout$strain%in%condition,]\n",
    "    else:\n",
    "        l = layout\n",
    "    \n",
    "    # Split on trial\n",
    "    \n",
    "    trials = 0 # trials = split(l, l$trial)\n",
    "    \n",
    "#Function to load full dataset\n",
    "    \n",
    "feature_pattern = re.compile(r\"^([\\w]*)\")\n",
    "time_pattern = re.compile(r\"([\\d]*)[)]$\")\n",
    "\n",
    "def load_full_larvae_data(path_to_file):\n",
    "    \n",
    "    #Load\n",
    "    x = pd.read_csv(path_to_file, index_col = 0)\n",
    "    x.columns = [f'larva_{i}' for i in range(1,len(x.columns)+1)]\n",
    "    \n",
    "    #Extract features\n",
    "    x['feature'] = x.index.str.extract(feature_pattern)[0].tolist()\n",
    "    x['time'] = x.index.str.extract(time_pattern)[0].tolist()\n",
    "    \n",
    "    #Split on feature\n",
    "    f = x.groupby(['feature'])\n",
    "    \n",
    "    #Combine into individual larvae\n",
    "    trials = {}\n",
    "\n",
    "    for i in range(x.shape[1]-2):\n",
    "        y = [group.iloc[:,i].reset_index(drop=True).rename(name[0]) for name, group in f]\n",
    "        \n",
    "        y = pd.concat(y,axis=1)\n",
    "        \n",
    "        diff_x = y[['mom_x', 'head_x', 'tail_x']].diff().values\n",
    "        diff_y = y[['mom_y', 'head_y', 'tail_y']].diff().values\n",
    "        y['mom_theta'] = np.arctan2(diff_x[:, 0], diff_y[:, 0]) * (180 / np.pi)\n",
    "        y.loc[0, 'mom_theta'] = 0\n",
    "        y['head_theta'] = np.arctan2(diff_x[:, 1], diff_y[:, 1]) * (180 / np.pi)\n",
    "        y.loc[0, 'head_theta'] = 0\n",
    "        y['tail_theta'] = np.arctan2(diff_x[:, 2], diff_y[:, 2]) * (180 / np.pi)\n",
    "        y.loc[0, 'tail_theta'] = 0\n",
    "        \n",
    "        y['mom_vr'] = np.insert(np.diff(y['mom_theta'].values), 0, 0)\n",
    "        y['head_vr'] = np.insert(np.diff(y['head_theta'].values), 0, 0)\n",
    "        y['tail_vr'] = np.insert(np.diff(y['tail_theta'].values), 0, 0)\n",
    "        \n",
    "        y['time'] = np.arange(1,len(y)+1)\n",
    "        \n",
    "        trials[x.columns[i]] = y\n",
    "\n",
    "        \n",
    "    return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a61130",
   "metadata": {},
   "source": [
    "# Load and clean data (collected at 10hz)\n",
    "###### This step takes the longest, progress wil be indicated below the box with the number file being loaded out of the total number of files to be loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9008a62",
   "metadata": {},
   "source": [
    "## Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate empty dict to load files into\n",
    "controls = {}\n",
    "\n",
    "###&&&Change_path&&&###\n",
    "#Set working directory to where the behavior files are (you'll need to change the path/name to match your data)\n",
    "os.chdir(os.path.join(wd,directory_control))\n",
    "\n",
    "#List files in directory\n",
    "files = os.listdir(os.getcwd())\n",
    "\n",
    "#Loop through and load files into 'controls' dictionary\n",
    "for i in range(len(files)):\n",
    "    print(i+1, \"out of\", len(files))\n",
    "    controls[i] = load_full_larvae_data(files[i])\n",
    "    \n",
    "#Combine\n",
    "combined_controls = {}\n",
    "for i in range(len(controls)):\n",
    "    for key, value in controls[i].items():\n",
    "        if key not in combined_controls:\n",
    "            combined_controls[key] = value\n",
    "        else:\n",
    "            new_key = key + \"_\" + str(i)\n",
    "            combined_controls[new_key] = value\n",
    "            \n",
    "#Change names and add larvae\n",
    "controls = {\"control_\" + str(i): v for i, v in enumerate(combined_controls.values(), start=1)}\n",
    "for k, v in controls.items():\n",
    "    # Remove rows with NA values\n",
    "    controls[k] = controls[k].dropna(axis=0, how='any')\n",
    "\n",
    "    # Remove NAs\n",
    "    controls[k] = controls[k].dropna()\n",
    "\n",
    "    # Add larvae\n",
    "    controls[k]['larvae'] = k\n",
    "    \n",
    "#Turn into matrix\n",
    "controls = pd.concat(controls.values(), keys= controls.keys())\n",
    "controls.index = [f'{i}.{j+1}' for i, j in controls.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ca0d91",
   "metadata": {},
   "source": [
    "## Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea87e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate empty dict to load files into\n",
    "experimentals = {}\n",
    "\n",
    "###&&&Change_path&&&###\n",
    "#Set working directory to where the behavior files are (you'll need to change the path/name to match your data)\n",
    "os.chdir(os.path.join(wd,directory_experimental))\n",
    "\n",
    "#List files in directory\n",
    "files = os.listdir(os.getcwd())\n",
    "\n",
    "#Loop through and load files into 'experimentals' dictionary\n",
    "for i in range(len(files)):\n",
    "    print(i+1, \"out of\", len(files))\n",
    "    experimentals[i] = load_full_larvae_data(files[i])\n",
    "    \n",
    "#Combine\n",
    "combined_experimentals = {}\n",
    "for i in range(len(experimentals)):\n",
    "    for key, value in experimentals[i].items():\n",
    "        if key not in combined_experimentals:\n",
    "            combined_experimentals[key] = value\n",
    "        else:\n",
    "            new_key = key + \"_\" + str(i)\n",
    "            combined_experimentals[new_key] = value\n",
    "            \n",
    "#Change names and add larvae\n",
    "experimentals = {\"experimental_\" + str(i): v for i, v in enumerate(combined_experimentals.values(), start=1)}\n",
    "for k, v in experimentals.items():\n",
    "\n",
    "    # Remove rows with NA values\n",
    "    experimentals[k] = experimentals[k].dropna(axis=0, how='any')\n",
    "\n",
    "    # Remove NAs\n",
    "    experimentals[k] = experimentals[k].dropna()\n",
    "\n",
    "    # Add larvae\n",
    "    experimentals[k]['larvae'] = k\n",
    "    \n",
    "\n",
    "#Turn into matrix\n",
    "experimentals = pd.concat(experimentals.values(), keys= experimentals.keys())\n",
    "experimentals.index = [f'{i}.{j+1}' for i, j in experimentals.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a07ed2",
   "metadata": {},
   "source": [
    "## Combine and Clean Controls and Experimentals DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine controls and experimentals into one matrix\n",
    "y3 = pd.concat([controls, experimentals])\n",
    "\n",
    "#Remove NA rows\n",
    "y3 = y3.dropna(axis=0, how='any')\n",
    "\n",
    "#Remove NAs\n",
    "y3 = y3.dropna()\n",
    "\n",
    "#Remove not well oriented rows\n",
    "y3 = y3[y3['is_well_oriented'] != 0]\n",
    "y3 = y3[y3['is_coiled'] != 1]\n",
    "\n",
    "#Convert to proper data types\n",
    "columns = y3.columns\n",
    "for i in range(len(y3.columns)-1):\n",
    "    y3[columns[i]] = y3[columns[i]].astype('float')\n",
    "    \n",
    "#Filter to desired features\n",
    "feat = pd.DataFrame({'area' : y3['area'],\n",
    "                 'bending' : abs(y3['bending']-180),\n",
    "                 'velocity' : y3['velocity'],\n",
    "                 'spine' : y3['spine_length'],\n",
    "                 'radius_1' : y3['radius_1'],\n",
    "                 'radius_2' : y3['radius_2'],\n",
    "                 'radius_3' : y3['radius_3'],\n",
    "                 'perimeter' : y3['perimeter'],\n",
    "                 'head_vr' : abs(y3['head_vr']),\n",
    "                 'mom_vr' : abs(y3['mom_vr']),\n",
    "                 'tail_vr' : abs(y3['tail_vr']),\n",
    "                 'dist' : y3['dst_to_origin']})\n",
    "\n",
    "#Remove NA rows\n",
    "feat = feat.dropna(axis=0, how='any')\n",
    "\n",
    "#Remove NAs\n",
    "feat = feat.dropna()\n",
    "\n",
    "#Split\n",
    "s = feat.groupby(feat.index.str.split('.').str[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35000d42",
   "metadata": {},
   "source": [
    "### Save Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d39fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{wd}/{experiment_name}_features_individual_trials_{experiment_date}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(s, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c158a",
   "metadata": {},
   "source": [
    "## Filtering and Detrending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396589ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Here, we required trials to have at least 250 timepoints\n",
    "#Select only trials longer than n (if desired)\n",
    "n = 250 #min timepoints\n",
    "s = s.filter(lambda group: len(group) > n)\n",
    "s = s.groupby(s.index.str.split('.').str[0])\n",
    "\n",
    "#Filter on distance traveled (if desired)\n",
    "n = 50 #min distance\n",
    "s = s.filter(lambda group: group['dist'].max(skipna=True) > 50)\n",
    "s = s.groupby(s.index.str.split('.').str[0])\n",
    "\n",
    "#Detrend size values (if desired; generally recommended since, as larvae get further from center, \n",
    "#their recorded size varies)\n",
    "\n",
    "def transform_group(group):\n",
    "    group['area'] = group['area'] / group['area'].rolling(10, center=True).mean()\n",
    "    group['area'].fillna(group['area'].median(), inplace=True)\n",
    "    \n",
    "    group['perimeter'] = group['perimeter'] / group['perimeter'].rolling(10, center=True).mean()\n",
    "    group['perimeter'].fillna(group['perimeter'].median(), inplace=True)\n",
    "    \n",
    "    group['radius_1'] = group['radius_1'] / group['radius_1'].rolling(10, center=True).mean()\n",
    "    group['radius_1'].fillna(group['radius_1'].median(), inplace=True)\n",
    "    \n",
    "    group['radius_2'] = group['radius_2'] / group['radius_2'].rolling(10, center=True).mean()\n",
    "    group['radius_2'].fillna(group['radius_2'].median(), inplace=True)\n",
    "    \n",
    "    group['radius_3'] = group['radius_3'] / group['radius_3'].rolling(10, center=True).mean()\n",
    "    group['radius_3'].fillna(group['radius_3'].median(), inplace=True)\n",
    "    \n",
    "    group['spine'] = group['spine'] / group['spine'].rolling(10, center=True).mean()\n",
    "    group['spine'].fillna(group['spine'].median(), inplace=True)\n",
    "    \n",
    "    return group\n",
    "\n",
    "s = s.apply(transform_group).reset_index(level=0, drop=True)\n",
    "s = s.groupby(s.index.str.split('.').str[0])\n",
    "\n",
    "#Calculate z-scores\n",
    "def scale_group(group):\n",
    "    scaler = StandardScaler()\n",
    "    group.iloc[:, :11] = scaler.fit_transform(group.iloc[:, :11])\n",
    "    return group\n",
    "\n",
    "s = s.apply(scale_group).reset_index(level=0, drop=True)\n",
    "y = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774b964",
   "metadata": {},
   "source": [
    "### Save Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0656ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{wd}/{experiment_name}_features_{experiment_date}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(s, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b09b30",
   "metadata": {},
   "source": [
    "## Running PCA on input  parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Since some of the behavioral parameters that were tracked are correlated, PCA can help reduce the \n",
    "#number of dimensions needed\n",
    "\n",
    "y_without_dist = y.loc[:, y.columns != 'dist']\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "y_scaled = scaler.fit_transform(y_without_dist)\n",
    "\n",
    "# Run PCA\n",
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(y_scaled)\n",
    "\n",
    "cum_explained_var = []\n",
    "for i in range(0, len(pca.explained_variance_ratio_)):\n",
    "    if i == 0:\n",
    "        cum_explained_var.append(pca.explained_variance_ratio_[i])\n",
    "    else:\n",
    "        cum_explained_var.append(pca.explained_variance_ratio_[i] + \n",
    "                                 cum_explained_var[i-1])\n",
    "\n",
    "cum_explained_var = np.array(cum_explained_var)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents, \n",
    "                          columns=['PC%s' % _ for _ in range(1,len(y_without_dist.columns)+1)],\n",
    "                          index = y_without_dist.index)\n",
    "\n",
    "#Plot\n",
    "plt.plot(np.arange(1, len(cum_explained_var) + 1), cum_explained_var * 100, marker='o')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('n PCs')\n",
    "plt.ylabel('Variation explained')\n",
    "plt.ylim(0, 100)\n",
    "plt.axhline(y=90, linestyle='--', color='gray', linewidth=2)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ec7f0",
   "metadata": {},
   "source": [
    "###### input the number of points that explain 90 percent of the variation (up to and including the point past the dashed grey line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75fc212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get pca loadings\n",
    "##The first 8 pcs tend to capture the majority of variation in larval input parameters\n",
    "s = principalDf.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a41d22",
   "metadata": {},
   "source": [
    "### Save Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c552ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{wd}/{experiment_name}_PCA_{experiment_date}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(s, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a68e1a",
   "metadata": {},
   "source": [
    "# Get feature windows of desired size and run UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get windows (here using a window size of 8, recommended for larval behavior)\n",
    "#This step takes a while to load, but will show progress after about a minute or two, depending on data size could take several minutes overall\n",
    "\n",
    "win = tf.get_windows(s, window_size = 8).transpose()\n",
    "\n",
    "\n",
    "#Run umap\n",
    "u = umap.UMAP(verbose=True).fit_transform(win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db958dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract UMAP 2d layout\n",
    "layout = pd.DataFrame({'x': u[:, 1], 'y': u[:, 0]}, index = win.index)\n",
    "\n",
    "#Bin (64x64 grid ('n_bins' option) recommended)\n",
    "layout = tf.bin_umap(layout, n_bins = 64)[\"layout\"]\n",
    "\n",
    "#Add trial\n",
    "layout['trial'] = layout.index.str.split('.').str[0]\n",
    "\n",
    "#Add time\n",
    "layout['time'] = layout.index.str.split('.').str[1]\n",
    "\n",
    "#Add id\n",
    "layout['id'] = layout['trial'] + \"_\" + layout['time']\n",
    "\n",
    "#Add condition\n",
    "layout['condition'] = layout['trial'].str.split(\"_\").str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86123fbd",
   "metadata": {},
   "source": [
    "### Save Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6028436",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{wd}/{experiment_name}_windowsize_8_{experiment_date}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(layout, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07cc6ec",
   "metadata": {},
   "source": [
    "# Analyzing UMAP behavior space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbb10f",
   "metadata": {},
   "source": [
    "### Open saved data (this step can be accessed at any point after analysis above is run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154bfaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load layout (wherever and with whichever name you decide to use)\n",
    "with open(f\"{wd}/{experiment_name}_windowsize_8_{experiment_date}.pkl\", \"rb\") as f:\n",
    "    layout = pickle.load(f)\n",
    "\n",
    "    #Load features (wherever and with whichever name you decide to use)\n",
    "with open(f\"{wd}/{experiment_name}_features_{experiment_date}.pkl\", \"rb\") as f:\n",
    "    features = pickle.load(f)\n",
    "\n",
    "#Match features to the rownames of the layout file (i.e. making sure they represent the same timepoints)\n",
    "features = features.reindex(layout.index)\n",
    "\n",
    "#Combine\n",
    "layout = pd.concat([layout, features], axis=1)\n",
    "\n",
    "#Correlation between layout xy position and features\n",
    "cor_x = layout.iloc[:, 10:23].apply(lambda col: np.corrcoef(layout['x'], col)[0, 1])\n",
    "\n",
    "# Correlation between layout y position and features\n",
    "cor_y = layout.iloc[:, 10:23].apply(lambda col: np.corrcoef(layout['y'], col)[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477683f",
   "metadata": {},
   "source": [
    "### Scatter Plot 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2d layout of UMAP behavior space\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax1 = sns.scatterplot(x='x', y='y', data=layout, alpha=0.2, s = 5, color = 'grey')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd26c14",
   "metadata": {},
   "source": [
    "### Line Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebfa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2d structure of UMAP behavior space\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax2 = sns.lineplot(x='x', y='y', data=layout, sort=False, linewidth=0.004, color='grey', estimator=None, alpha = 1)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa65f3d",
   "metadata": {},
   "source": [
    "### Density Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9549129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2d probability density function of behavior space\n",
    "\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax3 = sns.kdeplot(x='x', y='y', data=layout, fill=True, cmap=\"YlOrRd\", bw_adjust = 1, levels = 10)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee671ea2",
   "metadata": {},
   "source": [
    "# Analyzing input features (e.g. radius, spine length) as a function of behavior space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f60a89",
   "metadata": {},
   "source": [
    "### Plot Behavior Space by Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60facc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting features as means of layout bins\n",
    "\n",
    "lst_feats = layout.columns.values.tolist()[10:22]\n",
    "\n",
    "# Split the DataFrame into bins based on 'xy_new'\n",
    "x = layout.groupby('xy_new')\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(3, 4, figsize=(20, 15), constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plotting features as means of layout bins\n",
    "for idx, i in enumerate(range(10, 22)):\n",
    "    z = x.apply(lambda group: round(group.iloc[:, i].mean(), 2))\n",
    "    z = z.clip(z.quantile(0.01), z.quantile(0.99))\n",
    "\n",
    "    if z.min() > 0 or z.max() < 0:\n",
    "        norm = Normalize(vmin=z.min(), vmax=z.max())\n",
    "        cmap = 'Reds'\n",
    "    else:\n",
    "        norm = Normalize(vmin=z.min(), vmax=z.max())\n",
    "        cmap = 'RdBu_r'\n",
    "    \n",
    "    xy_coords = [coord.split('_') for coord in z.index]\n",
    "    x_coords = [int(coord[0]) for coord in xy_coords]\n",
    "    y_coords = [int(coord[1]) for coord in xy_coords]\n",
    "\n",
    "    sns.scatterplot(x=x_coords, y=y_coords,hue=z,palette=sns.color_palette(cmap, as_cmap=True), hue_norm=norm,\n",
    "                    s=30,legend=False, ax=axs[idx], alpha = 1)\n",
    "    axs[idx].set_title(lst_feats[idx], fontsize = 20)\n",
    "    axs[idx].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b1760",
   "metadata": {},
   "source": [
    "# Population-level variation (via time spent per bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b81295",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = {k: v for k, v in layout.groupby('condition')}\n",
    "\n",
    "b = layout['xy_new'].unique()\n",
    "xy_coords = [coord.split('_') for coord in b]\n",
    "x_coords = [int(coord[0]) for coord in xy_coords]\n",
    "y_coords = [int(coord[1]) for coord in xy_coords]\n",
    "\n",
    "props = {}\n",
    "\n",
    "for cond_name, df in condition.items():\n",
    "    z = []\n",
    "    total_rows = len(df)\n",
    "    for bins in b:\n",
    "        proportion = (df['xy_new'] == bins).sum() / total_rows\n",
    "        z.append(proportion)\n",
    "    props[cond_name] = np.round(np.array(z) / np.max(z), 2)\n",
    "\n",
    "values = np.round(np.arange(0, 1.01, 0.01), 2)\n",
    "cmaps = [\"Oranges\", \"Purples\"]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for idx, (cond_name, values) in enumerate(props.items()):\n",
    "\n",
    "    rounded_vals = np.round(values, 2)\n",
    "\n",
    "    sns.scatterplot(x=x_coords, y=y_coords, hue = rounded_vals, palette=cmaps[idx], legend=False, ax=axs[idx], \n",
    "                    s = 40)\n",
    "    axs[idx].set_title(cond_name, fontsize=15, fontweight='normal')\n",
    "    axs[idx].axis('off')\n",
    "    \n",
    "\n",
    "cond_names = list(props.keys())\n",
    "d = props[cond_names[0]] - props[cond_names[1]]\n",
    "d = np.clip(d, -0.4, 0.4)\n",
    "\n",
    "diff_df = pd.DataFrame({\n",
    "    \"x\": x_coords,\n",
    "    \"y\": y_coords,\n",
    "    \"diff\": np.round(d, 2)})\n",
    "\n",
    "sns.scatterplot(data=diff_df, x=\"x\", y=\"y\", hue=\"diff\", palette=\"PuOr_r\", hue_norm=Normalize(vmin=-0.4, vmax=0.4),\n",
    "                s=40, legend=False, ax=axs[2])\n",
    "axs[2].set_title(\"Difference\", fontsize=15, fontweight='normal')\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5952b5",
   "metadata": {},
   "source": [
    "# Louvain clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a8f9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rebin\n",
    "layout = tf.bin_umap(layout, n_bins=50)[\"layout\"]\n",
    "\n",
    "#Make into graph\n",
    "edges = list(zip(layout['xy_new'].iloc[:-1], layout['xy_new'].iloc[1:]))\n",
    "g = nx.Graph()\n",
    "g.add_edges_from(edges)\n",
    "\n",
    "#Cluster with Louvain\n",
    "partition = community.best_partition(g) \n",
    "\n",
    "x_coords = [int(xy.split('_')[0]) for xy in partition.keys()]\n",
    "y_coords = [int(xy.split('_')[1]) for xy in partition.keys()]\n",
    "cluster_ids = list(partition.values())\n",
    "cluster_df = pd.DataFrame({\n",
    "    \"x\": x_coords,\n",
    "    \"y\": y_coords,\n",
    "    \"cluster\": cluster_ids})\n",
    "\n",
    "cluster_ids = sorted(set(partition.values()))\n",
    "n_clusters = len(cluster_ids)\n",
    "palette = sns.color_palette(\"Spectral\", n_colors=n_clusters)\n",
    "cluster_color_map = {cid: palette[i] for i, cid in enumerate(cluster_ids)}\n",
    "\n",
    "# Binned Louvain Cluster Plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(data=cluster_df, x=\"x\", y=\"y\", hue=\"cluster\", \n",
    "                palette=cluster_color_map, legend=False, s=30)\n",
    "plt.axis('off')\n",
    "plt.title(\"Binned Louvain Clusters\")\n",
    "plt.show()\n",
    "\n",
    "louv_layout = layout.copy()\n",
    "louv_layout['louvain_cluster'] = louv_layout['xy_new'].map(partition)\n",
    "\n",
    "# Points Louvain Cluster Plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(data=louv_layout, x=\"x\", y=\"y\", hue=\"louvain_cluster\", \n",
    "                palette=sns.color_palette(\"Spectral\", n_colors=len(set(partition.values()))), s=2, legend=False)\n",
    "plt.axis('off')\n",
    "plt.title(\"Louvain Clusters on UMAP Layout\")\n",
    "plt.show()\n",
    "\n",
    "s = {k: v for k, v in louv_layout.groupby('condition')}\n",
    "\n",
    "o = {}\n",
    "\n",
    "for cond_name, df in s.items():\n",
    "    trial_groups = df.groupby('trial')\n",
    "    o[cond_name] = [trial['louvain_cluster'].value_counts(normalize=True) for _, trial in trial_groups]\n",
    "\n",
    "height = math.ceil(n_clusters / 3)\n",
    "width  = 3\n",
    "\n",
    "fig, axs = plt.subplots(height, width, figsize=(20, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Time spent by condition per region\n",
    "for i, cluster_id in enumerate(cluster_ids):\n",
    "    \n",
    "    plot_df = pd.DataFrame([\n",
    "        {'condition': cond, 'occupancy': occ.get(cluster_id, np.nan)}\n",
    "        for cond, trials in o.items()\n",
    "        for occ in trials if cluster_id in occ\n",
    "    ])\n",
    "\n",
    "    sns.violinplot(data=plot_df, x='condition', y='occupancy', ax=axs[i], color=cluster_color_map[cluster_id])\n",
    "\n",
    "    axs[i].set_ylim(0, 0.6)\n",
    "    axs[i].set_ylabel('% time spent')\n",
    "    axs[i].set_xlabel('')\n",
    "\n",
    "    # Kruskal-Wallis test\n",
    "    grouped = plot_df.groupby('condition')['occupancy'].apply(list)\n",
    "    pval = kruskal(*grouped).pvalue if len(grouped) > 1 else np.nan\n",
    "    axs[i].set_title(f'p = {pval:.2g}', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "feature_cols = louv_layout.columns[10:21]\n",
    "feats = [louv_layout[louv_layout['louvain_cluster'] == cid][feature_cols] for cid in cluster_ids]\n",
    "means = [df.mean() for df in feats]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(height, width, figsize=(20, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Analyzing feature distributions in Louvain clusters\n",
    "for i, (cluster_id, mean_vec) in enumerate(zip(cluster_ids, means)):\n",
    "    mean_df = mean_vec.reset_index()\n",
    "    mean_df.columns = ['feature', 'mean_value']\n",
    "    \n",
    "    sns.barplot(data=mean_df, x='feature', y='mean_value', ax=axs[i],color=cluster_color_map[cluster_id])\n",
    "\n",
    "    axs[i].set_title(f'Cluster {cluster_id}')\n",
    "    axs[i].set_ylabel('Mean Z-Score')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
